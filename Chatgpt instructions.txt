Roadmap for Completion by Module
To address the gaps, below is a step-by-step roadmap organized by key modules. This plan will guide completion of the project and ensure all necessary pieces (pricing, hedging, risk analysis) are in place:
1. Pricing & Calibration Module
Objective: Finalize model calibrations and compute the fair cost of guarantees under each model.
Extend Calibration to All Models: Leverage the existing calibration framework to ensure each model‚Äôs parameters are market-consistent. GBM can be calibrated by matching its volatility to ATM implied vol for a given tenor (or simply use an appropriate historical vol). Heston calibration is in place; run it for multiple dates to sanity-check stability of parameters (or use the provided 2018-06-01 snapshot results as final). For the rough volatility model, consider calibration or literature values ‚Äì for instance, use market implied variance term structure or autocorrelation to choose H (e.g. H ~0.1 as given) and Œ∑. If direct calibration of rough vol is too advanced, clearly state the assumptions for its parameters.
Price the RILA Guarantee (Closed-Form/Replication): Before jumping into simulations, analytically determine the payoff‚Äôs composition. A single-period RILA payoff with floor = 90% and cap = 150% of initial can be decomposed into vanilla options: essentially long a put spread for the buffer and short a call spread for the cap. For validation, calculate the theoretical present value of the payoff using Black‚ÄìScholes (for GBM) or Heston pricing:
Compute the cost of providing the downside buffer: e.g. price a put option struck at 90% of initial (this covers all losses beyond 10% drop) ‚Äì or a put spread if losses beyond some extreme are not covered.
Compute the cost of the upside cap: price a call option struck at 150% of initial (this represents the foregone upside above cap, which the insurer effectively gains by shorting a call).
Sum these appropriately (with the long underlying exposure of the account) to get the net liability value. This analytical check is useful to ensure the simulation approach is yielding correct prices.
Risk-Neutral Monte Carlo Pricing: Using the risk-neutral simulations already being done (drift = r‚Äìq, then discount), calculate the expected present value of payouts for each model as a baseline. For example, take the average of discounted_accounts in the current outputs (this should equal the fair premium the insurer should charge, ignoring fees) and compare across models. Verify that under GBM, the simulation‚Äôs average payout matches the Black‚ÄìScholes analytical price from above. Document these prices ‚Äì they represent the cost of the guarantee and can be compared to fees charged (if the product charges, say, a 1% annual fee, is that sufficient? You can show the fee present value vs. guarantee cost).
Calibrate Risk-Neutral vs Real-World Parameters: Clarify if simulations for capital are done under the real-world measure (with an equity risk premium) or under risk-neutral. Currently Œº = r‚Äìq is used (risk-neutral). For hedging and pricing, that‚Äôs appropriate. For capital adequacy (which regulators often do under real-world with conservative assumptions), we might need to introduce a real-world drift (e.g. Œº_real = r‚Äìq + Œª). Decide on this approach. If focusing on risk-neutral only, justify it (perhaps assuming the worst-case risk-neutral as a conservative measure).
Deliverable from this module: Calibrated model parameters for GBM, Heston, (possibly rough vol) and baseline guarantee price calculations under each model. This sets the stage for hedging by confirming the liability‚Äôs fair value.
2. Scenario Simulation Module
Objective: Generate simulation scenarios for underlying indices and guarantee payouts, in a flexible, reusable way, to feed into hedging analysis.
Refactor Simulation Code into Functions: Currently, simulations for each model are done in separate scripts with some repetition. Create a unified function or set of functions for simulation, e.g. simulate_paths(model, params, n_paths, horizon, dt) that can generate price paths given a model type. Leverage the existing logic:
GBM: use vectorized log-normal steps
GitHub
.
Heston: use Euler steps with two correlated randoms
GitHub
.
Rough Vol: generate fractional Brownian once per path then simulate as done
GitHub
.
Parameterize these so that number of steps, number of paths, and model parameters can be easily varied (this will help in sensitivity analysis later).
Ensure Reproducibility and Performance: Continue using set random seeds for reproducibility in analysis. Consider the runtime ‚Äì 10,000 paths * 252*7 steps for Heston and rough vol is heavy; if needed, optimize (vectorize where possible, or use fewer paths initially for testing). Possibly save random seeds or the generated random series so that hedging strategies see the same market scenarios across different hedging assumptions (for apples-to-apples comparisons).
Simulate Under Both Hedged and Unhedged Conditions: Initially, generate all paths for the underlying process only (stock index paths). These will be reused for hedging. For the unhedged liability (already done: final account distribution), make sure to store the results (final payouts, etc.). Then, these same underlying paths will be input to the hedging module to simulate hedging P&L. By using identical scenarios, we can directly measure the benefit of hedging.
Incorporate Real-World vs Risk-Neutral Paths: If a separate set of real-world scenarios is desired (for capital requirement per regulations), the simulation module should allow using a different drift. This can be as simple as toggling Œº = r‚Äìq (risk-neutral) vs Œº = r‚Äìq+equity_premium (real-world). This will affect the distribution of outcomes (higher mean, broader distribution under real-world). Clearly mark scenario sets as such to avoid confusion.
Deliverable: A robust simulation engine that can output underlying price paths under different models and assumptions. These paths feed into both payoff calculation (liability) and the hedging simulation, ensuring consistency.
3. Dynamic Hedge Modeling Module
Objective: Implement the dynamic hedging strategy for the RILA, and simulate the hedger‚Äôs performance on each scenario path.
Choose Hedging Instruments: For a delta-hedging approach, the primary hedge instrument will be the underlying index (or a proxy like futures on the S&P 500) to cover equity risk. Since the product has a nonlinear payoff (due to the put-like buffer and call-like cap), a single underlying position won‚Äôt perfectly hedge; however, delta hedging can be used, rebalancing the index position to match the instantaneous delta of the guarantee. Initially plan to hedge with the underlying only (delta hedge), as that is simplest and typically how insurers hedge VA guarantees (they often do dynamic delta + perhaps vega hedge with options, but we can start with delta-only).
Compute Guarantee Delta: Derive an expression or method for the delta of the RILA payoff at any time before maturity. This can be complex because the payoff has path dependency if annual resets are involved. For simplicity, consider first the point-to-point structure: the payoff at maturity is 
min
‚Å°
(
max
‚Å°
(
ùëÜ
ùëá
,
0.9
ùëÜ
0
)
,
1.5
ùëÜ
0
)
min(max(S 
T
‚Äã
 ,0.9S 
0
‚Äã
 ),1.5S 
0
‚Äã
 ). The delta of this payoff at time T is either 0 (in the flat regions) or 1 (when not in the capped or floored region), effectively like a digital switch at certain boundaries. But for dynamic hedging, we need the delta at each time t < T. A practical approach: use scenario Greeks ‚Äì at each time step and for each path scenario, estimate the derivative of the remaining payoff w.r.t. S. This can be done by bumping the underlying slightly and re-evaluating the payoff (Monte Carlo within Monte Carlo), or (more straightforward) by recognizing that the hedge strategy for a payoff that is a combination of vanilla options is to hold those option-equivalent positions:
Replication approach: at inception, the RILA payoff = long 0.9-strike put + short 1.5-strike call + an initial stock position. Over time, as the underlying moves, the delta of the long put and short call can be computed using Black‚ÄìScholes formulas (with time to maturity decreasing). One could dynamically hedge by holding delta = (Œî_put ‚Äì Œî_call + Œî_stock). Here Œî_stock is 1 for the full exposure of the account value (the policy is essentially invested in the index). So effectively, the insurer‚Äôs liability delta = 1 (base account) + Œî_put (which is positive) ‚Äì Œî_call (positive), so >1 in downturns, <1 in upturns.
Implement a function to calculate option deltas for the put and call components under the current model. E.g., if using Black‚ÄìScholes for hedge ratios: Œî_put = ‚ÄìN(‚Äìd1) and Œî_call = N(d1). Use current underlying level, remaining time, and an implied vol (from model or calibration) to compute these. If using Heston or rough vol, using BS delta might still be a reasonable hedge proxy, or use local vol approximation.
For the annual-reset variant, the guarantee resets each year, which complicates delta calculation (the payoff in the first year is separate from subsequent years). In that case, hedge year by year: at the start of each year, the liability resets (previous buffer outcome locked in, new guarantee starts). The delta for the ongoing year‚Äôs participation can be computed similarly (cap and buffer on a 1-year forward basis).
Hedging Algorithm Implementation: With a method for delta in hand, simulate the hedge trading:
Start with an initial portfolio consisting of: short the RILA liability (implicitly) and a hedge position (e.g. some number of units of the underlying and possibly cash). At inception, the hedge portfolio value should equal the liability fair value (for a self-financing hedge). For example, initialize with the cost of the put minus the call (the insurer collects premium or sets aside capital equal to the cost of hedging).
At each time step (e.g. daily or monthly), update the hedge position: calculate the liability‚Äôs delta at that time, and adjust the holdings of the underlying to match that delta. This will involve buying or selling Œî_underlying shares. Record the cost of trades (if considering transaction costs later, incorporate bid-ask or a cost per trade, but initially assume frictionless trading).
Any difference between the hedge portfolio value and the liability value at maturity will be the hedging error (profit or loss to the insurer). The simulation should track the cash account as well ‚Äì e.g., when the hedge requires selling stock, cash is received and put in a bank account earning risk-free interest, etc., to properly track P&L. Essentially perform a daily P&L reconciliation:
Begin step with some stock and cash.
Underlying moves according to the scenario path. Liability value changes accordingly (but the insurer doesn‚Äôt pay until maturity; however, the mark-to-market of the liability can be computed theoretically for P&L tracking).
End of day, rebalance to new delta: determine how many shares to buy/sell to achieve new delta, adjust cash for the cost.
Continue until maturity, then compute final hedge portfolio value ‚Äì liability payoff. Ideally, this is zero (perfect hedge); any deviation is the hedging error.
Implement this simulation for each path (10,000+ scenarios). This is computationally heavy but feasible with vectorization. You might vectorize by computing deltas for all paths at once at each timestep (using array operations for d1 etc.). If that‚Äôs complex, a loop per path is fine for clarity, and perhaps run on a reduced set if needed to test.
Alternative Hedge Strategies: If time permits, consider hedging not only with the underlying but also with static options. For example, an alternative hedging approach could be to buy a put spread and sell a call spread at inception to cover the buffer and cap (essentially replicating the payoff from the start) and then just delta-hedge the difference in a simpler manner. This could potentially yield smaller errors if the vol model is correct, at the expense of initial cost. However, this may be beyond the main scope ‚Äì the primary required analysis is likely delta hedging since it‚Äôs a common strategy and reveals model risk when actual vol doesn‚Äôt match assumptions.
Incorporate Stochastic Vol in Hedging: Recognize that under Heston or rough vol, the underlying‚Äôs volatility is random, so a delta-only hedge may be imperfect (there will be vega risk). The hedge strategy above (delta hedging assuming BS deltas) will likely show residual errors due to volatility swings. This is actually an interesting result to capture (hedging error distribution might be wider under rough vol than under BS assumptions, for example). Keep the hedge strategy consistent across models for comparison (e.g. use BS-based delta hedging for all, or for Heston perhaps use Heston‚Äôs delta if known). This will highlight the model risk and volatility risk left unhedged.
Deliverable: A hedging simulation function or script that outputs the profit/loss for each scenario path. Likely an array of hedging P&L outcomes for all simulated paths is produced, which can then be analyzed statistically. This is the most crucial part to get right, as it feeds directly into the risk analysis.
4. Hedging Error Analysis Module
Objective: Analyze the results of the hedging simulation to evaluate effectiveness and identify risk drivers.
Distribution of Hedging P&L: Using the output from the hedge module, compute summary statistics for the hedger‚Äôs profit/loss distribution (which is essentially ‚Äì(liability ‚Äì hedge portfolio) since a loss to the insurer occurs if the hedge falls short and the insurer must pay extra). Key metrics: mean P&L (should be near 0 if hedged on average and models align), standard deviation of P&L (measures hedge effectiveness ‚Äì lower is better), and tail losses (e.g. 95th percentile loss). Plot a histogram of the P&L outcomes similar to how it was done for liability outcomes
GitHub
, but centered around zero. This will visually show how many scenarios lead to small hedging errors vs. large shortfalls or gains.
Compare Hedged vs Unhedged Risk: Prepare a comparison of the unhedged liability distribution versus the post-hedging P&L distribution. For example, you could tabulate or plot the VaR before and after hedging: ideally, hedging should significantly reduce the tail risk. If the 5% VaR of the liability (present value) was, say, ‚Äì$200 (meaning a $200 loss on $1000 initial in worst 5% cases), after hedging it might reduce to a much smaller number if the hedge is effective. Quantify this improvement. It may be useful to show a side-by-side of key stats:
Unhedged: mean, œÉ, 99% VaR, CTE95.
Hedged: mean, œÉ, 99% VaR, CTE95.
This demonstrates the benefit of hedging in risk reduction. If some models (e.g. rough vol) still show large œÉ or heavy tails even after delta hedging, note that ‚Äì this indicates model risk or that delta hedging alone doesn‚Äôt cover extreme moves (perhaps gamma/vega risks).
Examine Extreme Scenarios: Investigate a few worst-case scenarios in detail to understand why hedging failed there. For instance, identify the scenario path that resulted in the largest loss for the hedger; plot the path of the underlying and possibly the path of the hedge portfolio value vs liability. It might show, for example, a scenario where the market dropped very sharply (gap down) or volatility spiked, causing the delta-hedge to be inadequate. Documenting these scenarios qualitatively can provide insight (e.g. ‚Äúin the worst 1% scenarios, the market crashed more than the model anticipated, and the put options were deep in-the-money ‚Äì the static delta hedge couldn‚Äôt fully cover the jump‚Äù). This ties the quantitative results to real-world interpretations (like 1987-style crash risk, etc.).
Model Comparison: If multiple models are used (GBM vs Heston vs Rough), compare how hedging errors differ. Perhaps create a small table: Hedging P&L VaR/CTE under GBM assumptions vs under Heston vs under Rough. This addresses the question: does using a more sophisticated vol model (rough vol) lead to larger residual risk if hedging is done with simpler assumptions? Or conversely, if the insurer perfectly knew the model (say uses Heston delta under actual Heston world), how much risk remains from stochastic vol? These comparisons can be insightful for the discussion section of the dissertation.
Sensitivity Analyses: Time permitting, explore sensitivities: e.g. what if hedge rebalancing is less frequent (weekly vs daily)? What if the cap and buffer levels change? Or what if the insurer uses an implied volatility that is mismatched (model risk)? Each of these would affect hedging error. Even if not all can be done, mention these qualitatively as limitations or future work.
Deliverable: Statistical analysis and visualization of hedging performance. Expect outputs like histograms of P&L, tables of risk measures, and possibly example path illustrations. These results will directly feed into the capital requirement module.
5. Capital Requirement & Risk Metrics Module
Objective: Determine the capital needed to support the product under worst-case scenarios, using industry-standard metrics (VaR, CTE), both with and without hedging.
Compute Tail Metrics (VaR, CTE): Building on the distributions obtained:
Calculate VaR at relevant levels (e.g. 95%, 99% for one-year horizon if that‚Äôs the capital horizon, or over the contract term). Since we have the distribution of present values of losses, a 99% VaR might represent ‚Äúthe insurer‚Äôs loss will not exceed $X in 99% of cases.‚Äù Use numpy or pandas to get these quantiles (5th, 1st percentile of outcomes, etc., already partially done in code for unhedged case).
More importantly, compute Conditional Tail Expectation (CTE) for the same levels. For example, CTE(95) is the average loss in the worst 5% scenarios. Implement this by taking the sorted P&L array, slicing the worst 5% and taking the mean. Do this for both the unhedged liability (which gives the capital needed if the insurer didn‚Äôt hedge at all) and the hedged P&L (capital needed with hedging in place). The difference will illustrate the capital relief due to hedging.
If the project is for regulatory application, you might focus on CTE(90) or CTE(95) as the metric for Required Capital. Ensure to present these clearly (e.g. ‚ÄúCTE(95) without hedging = $XYZ, with hedging = $ABC‚Äù).
Multi-Period Consideration: Often capital is assessed at a one-year horizon (with an assumption the firm can re-evaluate hedges yearly). However, given the product term is 6-7 years, one could also look at the distribution of cumulative losses over the entire term. Likely, the project will keep it simple and consider the final outcome distribution as we have been doing (which is effectively the N-year risk in one shot). Clarify the horizon of the risk metrics (if it‚Äôs final outcome in year 7, that‚Äôs fine ‚Äì it‚Äôs like assessing capital for the entire liability run-off). If needed by the brief, translate that into an annualized metric or ensure the interpretation is clear.
Capital Adequacy Evaluation: Based on the CTE results, discuss whether the current fee structure covers the risk. For instance, if the insurer charges a 1% annual fee, over 7 years that might equate to about 7% of initial premium in present value. If the CTE95 of losses is 15% of the account, then hedging plus fee still leaves a shortfall ‚Äì meaning additional capital (from equity) is needed. Quantify how much capital an insurer would need to hold (e.g. if initial policy reserve equals the fair value of guarantee, additional capital = CTE95 ‚Äì reserve, etc.). This ties the technical results back to ERM: how much buffer capital for worst-case is required and how hedging reduces that requirement.
Regulatory Angles: Optionally, frame the results in terms of regulatory metrics: e.g. ‚ÄúThe 95% CTE corresponds to the RBC C3 Phase II requirement for this product.‚Äù If familiar, you can align the language with solvency frameworks (not mandatory, but good for context).
Stress Testing: Recommend a brief stress test: e.g. what if an extreme Black Swan event happens (far beyond model assumptions)? While not simulated, mention qualitatively what capital or risk management response would be (this shows awareness of model limits ‚Äì especially since 2020 COVID crash or 1987 crash could be scenarios beyond typical VaR).
Documentation of Assumptions: Clearly list assumptions used in risk metrics ‚Äì e.g. no transaction costs, perfect rebalancing, continuous operation (no default), etc., so the capital calculations are interpreted in that light. This helps the dissertation readers understand limitations and that actual required capital might be higher if, for example, we included transaction costs or model misspecification.
Deliverable: A set of risk metrics and capital requirement figures, with explanation. This likely takes the form of a section in the dissertation with a table of VaR/CTE results and commentary on how much risk is mitigated by hedging and what residual extreme loss the insurer must be prepared for. Finally, consolidate these modules‚Äô results into a coherent story: starting from model calibration (market-consistent setup), to pricing (cost of guarantees), to hedging strategy (risk mitigation), and ending with the impact on capital (what‚Äôs the remaining risk). This end-to-end flow satisfies both the quantitative finance perspective and the enterprise risk management objective of the project.
Code Improvements & Best Practices Suggestions
Throughout the development of the above modules, the following coding improvements should be incorporated for robustness and clarity:
Generalize Parameters and Configuration: Avoid hard-coding values (like S0, sigma=0.2, specific dates, buffer=0.1, etc.) scattered in scripts. Instead, collect key parameters into either a configuration file or at least a top-of-script dictionary of constants. Even better, allow functions to accept these as arguments. This makes it easy to change scenario assumptions (e.g. try different cap levels or path counts) without modifying code internals. For example, the Heston simulation uses fixed parameters
GitHub
; these could be fed in from the calibration output or a config so that one can switch to a different calibrated date‚Äôs parameters quickly.
Modularize and Reuse Code: Refactor the code into reusable components. Create modules for models, simulation, payoff calculation, hedging, and metrics:
A Model class or functions for each model (GBM, Heston, Rough) encapsulating simulation step logic and any pricing formula.
A single function for RILA payoff that takes returns or path and applies buffer/cap, instead of duplicating logic in multiple scripts. This ensures consistency (and one place to fix if the payoff formula needs adjustment).
A hedging function that, given a set of underlying paths and product parameters, returns P&L outcomes. This keeps the main analysis script cleaner and allows testing the hedging routine in isolation.
By modularizing, the final notebook or script can simply call these functions in sequence (calibrate -> simulate -> hedge -> analyze), improving readability.
Code Cleanup and Consistency: Remove or resolve any redundant scripts (e.g., if 7_rila_under_gbm.py and the logic in 6_simulate_spx_gbm.py overlap, consolidate them). Ensure consistent naming conventions (some files use numbers in names which can confuse order). Clear out any debugging prints or placeholder code (the ‚úÖ prints are fine for logging progress, but make sure they don‚Äôt clutter output in final runs). Use comments to explain non-obvious formulas (for instance, clarify the rough volatility formula or the reason for using np.abs() on variance in Heston simulation
GitHub
 to avoid negative variances). This will help others (and future you) to follow the logic without misunderstanding.
Documentation and Usage Guide: Augment the repository with a README or documentation explaining how to run the simulations and what each module does. For example, provide instructions: ‚ÄúTo calibrate the Heston model, run 6_heston_calibrate.py ‚Äì it will output parameters to CSV
GitHub
. Then update the simulation config with these parameters and run 8_simulate_spx_heston.py to generate paths
GitHub
,‚Äù etc. Also, comment within the code for complex parts (like explaining the hedging strategy steps). Since this is an academic project, also ensure the dissertation text includes descriptions of the code‚Äôs methodology ‚Äì having clear code comments will make writing those sections easier.
Visualization and Plot Enhancements: Continue using plots to illustrate findings, and consider adding more:
Plot the implied vol fit for Heston vs market data for a snapshot (to show calibration quality) ‚Äì e.g. overlay model IV curve on market IV points for a few maturities.
Plot sample price paths (already done for Heston and rough vol) in the dissertation to show how different models produce different dynamics (rough vol might show more volatility clustering, etc.).
For hedging, plot the hedging error distribution and perhaps a cumulative distribution function (CDF) to better illustrate tail probabilities.
If investigating an extreme scenario, a time series plot of that scenario‚Äôs underlying price and the hedging portfolio value would be insightful (showing where the hedge broke down).
Ensure all plots have clear labels, titles, and saved to the Output folder with descriptive names (this is already followed in places
GitHub
).
If time permits, create a summary visualization: for instance, a bar chart comparing required capital (CTE) with vs without hedging, or under different models, which makes the risk reduction immediately clear to an audience.
By implementing these improvements, the code will be easier to maintain, the analysis will be reproducible, and the final dissertation will be supported by well-organized evidence. In conclusion, following this roadmap will complete the project‚Äôs objectives: it will produce a comprehensive analysis of dynamic hedging for RILA in stochastic volatility markets, complete with calibrated pricing, simulated hedging performance, and quantified risk metrics ‚Äì all presented in a clear, professional manner. The result will be a robust dissertation that not only demonstrates technical proficiency in quantitative finance but also provides practical insights into risk management for equity-linked insurance products.


1. Overview of Repository Structure and Modules
The repository is organized as a collection of standalone Python scripts, each corresponding to a specific step in the analysis pipeline. Below is an outline of the main modules and their roles:
Data Preparation:
1_merge_data.py ‚Äì Merges raw S&P 500 (SPX) options datasets from multiple years into a single CSV.
2_clean_and_prepare.py ‚Äì Cleans the merged options data (drops rows with missing key fields, computes option mid-prices and normalized strike) and saves a cleaned dataset.
riskfree_trim.py ‚Äì Filters a historical risk-free yield curve dataset to the date range 2018‚Äì2023 and exports it.
riskfree_clean_prepare.py ‚Äì Cleans the trimmed risk-free data (renames columns, retains only date, maturity, and rate) and saves a final interest rate curve file.
(Note: A dataset of implied dividend yields (SPX_Implied_Yield_Rates_2018_2023.csv) is referenced for drift calculations, presumably prepared separately.)
Implied Volatility Visualization:
2_plot_target_iv_surface.py ‚Äì Loads a one-day snapshot of SPX option quotes (e.g. 2018-06-01) and produces a 3D scatter plot of the market implied volatility surface (volatility vs. strike vs. days to maturity).
3_plot_iv_smile.py ‚Äì Plots 2D ‚ÄúIV smile‚Äù curves for selected dates, by filtering the cleaned dataset for ~30-day maturity call options on each date and plotting implied vol vs. strike. Multiple dates are processed in a loop, and each smile plot is saved to the output folder.
Stochastic Models Simulations:
6_simulate_spx_gbm.py ‚Äì Simulates SPX price paths under a simple Geometric Brownian Motion (GBM) model. This script also applies an ‚Äúadvanced‚Äù RILA logic with annual resets and fees to the simulated paths (see below), and plots the distribution of final account values.
8_simulate_spx_heston.py ‚Äì Simulates SPX paths under the Heston stochastic volatility model for a 7-year horizon. Ten sample paths are plotted, and all simulated paths are saved to CSV for later use in RILA payoff analysis.
10_simulate_spx_roughvol.py ‚Äì Simulates SPX paths under a rough volatility model. It generates a fractional Brownian motion for volatility with Hurst parameter H=0.1 for each path, and then simulates the price dynamics accordingly. A sample of paths is plotted, and all paths are saved to CSV.
RILA Payoff Modeling: (RILA = Registered Index-Linked Annuity, with buffered downside and capped upside)
7_rila_under_gbm.py ‚Äì Loads GBM-simulated paths and applies a single-term RILA payoff at the end of 7 years (10% downside buffer, 12% annual cap). It then discounts the final payouts to present value and plots the distribution of outcomes.
9_rila_heston.py ‚Äì Applies the single-term RILA payoff logic to the Heston-simulated paths (10% buffer, 50% upside cap). The distribution of discounted 7-year account values is plotted and summarized.
11_rila_under_roughvol.py ‚Äì Similar to the above, for the rough volatility simulated paths. It assumes no annual reset (reset_annually=False) and applies the buffer and cap to the 7-year return on each path, then plots the distribution of outcomes.
6_simulate_spx_gbm.py (mentioned above) ‚Äì In addition to simulating GBM, this script implements an advanced RILA logic with annual resets: each year‚Äôs return is subject to the cap and buffer, with an annual fee deducted. The account value is updated year by year and a final distribution of account values (after 6 years, in this script) is obtained. This provides a comparison to the single-term RILA results.
Model Calibration:
6_heston_calibrate.py ‚Äì Calibrates the Heston model parameters to a given market snapshot (by default, June 1 2018) using local optimization (SciPy L-BFGS-B). It defines a loss function based on the squared error between model and market implied volatilities, subject to reasonable parameter bounds and penalties for unphysical values (e.g. negative variance, rho not in (-1,1) range). The calibrated parameters are printed and saved to CSV.
6_heston_calibrate_de.py ‚Äì Performs a global calibration of Heston using SciPy‚Äôs Differential Evolution. The loss function is essentially the same as above, but with slightly adjusted integration settings for robustness. Parameter bounds are similar. After the global search, the script prints and saves the resulting parameters. (The naming conflict with the other ‚Äú6_‚Äù script suggests this was an alternative approach; it uses a different filename to avoid collision.)
Utilities and Helper Modules:
utils.py ‚Äì Provides utility functions for interest rates, including get_r_minus_q(date, yield_file, rate_file) to compute the risk-neutral drift (risk-free rate minus dividend yield) for a given date, and get_r_for_discounting(date, rate_file) to fetch an appropriate discount rate for present value calculations. These are used across the simulation and RILA scripts to ensure consistency in drift and discounting.
heston_pricing_utils.py ‚Äì Defines the Heston model‚Äôs characteristic function heston_characteristic_function(...) for use in pricing.
heston_pricing_carr_madan.py ‚Äì Implements the Carr-Madan Fourier pricing method to price European calls under Heston. This function is used in the calibration scripts to compute model option prices. It imports the characteristic function from heston_pricing_utils.py.
bs_utils.py ‚Äì Contains Black-Scholes related utilities. In particular, it provides bs_implied_vol(price, S0, K, T, r, q) to compute the implied volatility from a given call price. The calibration routines use this to convert model prices to implied vols for error calculation. (Likely implemented via iterative root-finding for the Black-Scholes formula; the details are not shown in the snippet, but its usage is evident.)
Overall, the repository is structured in a step-by-step manner, from data processing to exploratory visualization, simulation under various models, and finally calibration and payoff analysis. The script naming (numbered prefixes) generally indicates the intended sequence, although there is some overlap in numbering (e.g. ‚Äú6‚Äù used for multiple purposes) which could be a minor source of confusion. Each script is designed to be run independently, printing status messages (notably with ‚úÖ and ‚è≥ markers for progress) and saving outputs (CSV files or plots) to organized subfolders under Output/.
2. Correctness and Behavior of Each Module
Data Preprocessing: The data merging and cleaning scripts appear straightforward and correct. The merging (1_merge_data.py) simply concatenates yearly CSV files and confirms the combined row count. The cleaning script then parses dates, drops rows with missing critical fields (impl_volatility, bids, asks, etc.), and creates new columns for time-to-maturity in days/years, mid-market option price, and properly scaled strike. These operations are appropriate for preparing the option dataset. One observation is that dropping all rows with any missing values in those key columns could discard a significant number of entries if, for example, an implied vol is not computed but a price exists. In practice, however, implied vol is usually provided for valid quotes, so this likely only removes invalid or incomplete entries. The creation of mid_price and dividing strike_price by 1000 to get the true strike is correct given many option datasets quote strikes as scaled integers. The script successfully saves a cleaned CSV for downstream use. The risk-free rate data preparation is also handled in two stages: filtering and cleaning. The riskfree_trim.py script isolates the interest rate curve data for 2018‚Äì2023, ensuring only the relevant date range is kept (this is important to match the option data period). Then riskfree_clean_prepare.py renames the ‚Äúdays‚Äù column to ‚Äúmaturity_days‚Äù for clarity and strips the dataset down to date, maturity, and rate columns. It outputs a cleaned file (Interest_Rate_Curves_2018_2023_CLEANED.csv) used by other modules. These steps are correctly implemented and help avoid confusion (for example, renaming the column prevents misinterpreting ‚Äúdays‚Äù as calendar days vs. trading days). Both scripts print confirmations of their actions, aiding the user in verifying that the data was processed (e.g. number of filtered rows, successful saves). One minor point: the repository references an implied dividend yield data file (SPX_Implied_Yield_Rates_2018_2023.csv) for computing drift (get_r_minus_q). The code does not show how this file was generated. It‚Äôs presumably calculated from index forward prices or provided externally. Its absence in the code suggests that part of the data prep might have been done outside these scripts. For completeness, providing a script or description for obtaining the implied yields would be useful, though this does not affect the correctness of the existing code. IV Surface Visualization: The plotting scripts serve as sanity checks and exploratory analysis of the data. 2_plot_target_iv_surface.py reads a pre-saved snapshot (e.g. one day‚Äôs option chain) and uses a 3D scatter to visualize the implied vol surface. The use of color mapping to implied vol itself is a nice touch (volatility shown both on z-axis and by color). This script assumes the snapshot CSV (for the chosen date) is already prepared. Indeed, it prints the number of rows loaded and even shows the first few lines of the DataFrame as a quick verification, which is helpful. The plotting code itself is correct for a scatter plot; however, no surface interpolation is done. The result is a cloud of points ‚Äì which is acceptable for seeing the general shape, though if a smooth surface is desired, one might add grid interpolation. As written, it‚Äôs a quick way to confirm that the data looks reasonable (e.g. term structure and strike dependence of IV). The 3_plot_iv_smile.py script filters the full cleaned options dataset for options near 1-month maturity on specific dates, then plots the cross-section of implied vols versus strike for call options. This effectively produces a smile (or skew) for each date. The filtering uses maturity_days.between(25, 35) as a proxy for ~30-day options, which is a reasonable approach to get a consistent maturity bucket. It also restricts to calls (cp_flag == 'C') so that puts are not mixed in; this is appropriate because mixing calls and puts could double-count or confuse the smile shape (since S&P index options exhibit a skew where puts have higher IV). The code handles the case of no data (prints a warning and skips if the subset is empty for a given date), which is a good robustness feature (for example, if the date falls on a non-trading day or data is missing). Each plot is saved with a filename indicating the date, and the formatting (axes labels, grid) is reasonably done. The result is a series of scatter plots of implied vol vs. strike for different dates, useful for seeing how the smile shifts over time. The code is correct in behavior. One suggestion: adding a legend or annotation on each plot with the date (currently only in the title) could be helpful when looking at many saved figures, but this is a minor UX detail. Overall, the visualization scripts confirm data quality and provide qualitative insight, which is their purpose. Heston Model Simulation: The 8_simulate_spx_heston.py script appears to correctly implement the Euler discretization of the Heston SDEs. The approach is vectorized across paths, which is efficient. Instead of an inner loop over each simulation path, it uses NumPy arrays to evolve all paths in parallel at each time step. The correlation between the two Brownian motions is introduced by constructing W2 as a linear combination of Z1 and Z2 with correlation coefficient œÅ, which is the standard method to couple the stochastic variance and stock processes. Key details:
It initializes arrays for stock price S and variance V of shape (N+1, n_paths). All paths share the same time grid.
For each time step t, it updates variance according to $V_t = |V_{t-1} + \kappa(\theta - V_{t-1}),dt + \sigma_v \sqrt{V_{t-1}},\sqrt{dt},W2_{t-1}|$. The use of np.abs(...) ensures variance remains non-negative, a simple fix to avoid occasional negative variance due to Euler-Maruyama discretization error. This absolute value trick is a common pragmatic approach, though it can introduce slight bias (it would reflect variance at the boundary). Given a reasonably small time step (daily) and typical Heston parameters, this is acceptable and helps maintain stability.
The stock price is updated as $S_t = S_{t-1} \exp((\mu - 0.5V_{t-1})dt + \sqrt{V_{t-1}},\sqrt{dt},W1_{t-1})$, which is the correct Euler log-update for the asset under stochastic volatility.
The drift mu is obtained by get_r_minus_q for the start date (2018-01-03 in this case). This presumably gives a constant risk-neutral drift = r ‚Äì q (around a few percent annual). Using a constant drift over 7 years is a simplifying assumption (in reality the interest rate or dividend yield curve could change over time; here it likely picks the short rate or a relevant rate at the start). This is fine for simulation purposes, as the focus is on volatility effects.
After simulation, the script saves all simulated paths to a CSV and also plots a small sample (10 out of 10,000 paths). The plotting is handled nicely with grid and labels, and the output is saved as an image. The code prints a completion message with a ‚úÖ, indicating success.
The Heston simulation code is accurate in its implementation and produces the expected outputs. The use of vectorization means it should run reasonably fast even for 10,000 paths (since it only loops over 1764 time steps, and uses numpy operations for the 10k-path updates). One thing to note is that it uses the risk-neutral drift (r‚Äìq). This suggests the simulations are being used for pricing or risk-neutral scenario analysis (consistent with later discounting of outcomes). Indeed, for pricing a derivative like the RILA, using risk-neutral simulations and then discounting by $e^{-rT}$ should give the fair value (assuming the product payoff is handled appropriately). We will examine the payoff logic next. Rough Volatility Simulation: The rough volatility model implemented in 10_simulate_spx_roughvol.py is more novel. It generates a fractional Brownian motion (fBM) path for each simulation path to model the stochastic variance. Specifically, for each path i, it does:
Generate W_H(t) ~ fBM with Hurst $H=0.1$ over $[0, T]$ (T=7 years) using the Davies-Harte method.
Define the variance process as $v(t) = \xi_0 \exp!\Big(\eta,W_H(t) - \frac{1}{2}\eta^2 t^{2H}\Big)$. This formula ensures $E[\exp(\eta W_H(t) - \frac{1}{2}\eta^2 t^{2H})] = 1$, so $E[v(t)] = \xi_0$ (maintaining the initial variance level on average). It effectively produces a lognormal rough volatility path, where the covariance of log-vol decays with a roughness exponent $H$.
Simulate the asset path given this variance path: for each small interval, it draws an independent Brownian increment $dW$ for the asset and applies $S_{j+1} = S_j \exp((\mu - \frac{1}{2}v_t) dt + \sqrt{v_t},dW)$.
The approach is conceptually sound as a simple rough volatility model (sometimes called a lognormal fractional volatility model). However, the implementation uses nested Python loops: an outer loop over each path (10,000 iterations) and an inner loop over each time step (‚âà1764 steps). This double loop (‚àº17.6 million iterations) in pure Python is quite computationally heavy. In practice, this will be much slower than the Heston simulation. Depending on the efficiency of the FBM library call, the bottleneck might be partly in generating the fBM (which is likely implemented in C under the hood), but the inner loop for the asset path update could significantly slow things down. On correctness: the logic inside the loops is correct ‚Äì it uses the risk-neutral drift mu = r‚Äìq for consistency, and updates the price in small time steps with the given instantaneous variance. The script prints progress messages and on completion, saves both sample plot and the full paths to CSV. In summary, the rough volatility simulation should produce plausible paths with the heavy-tailed, persistent volatility fluctuations characteristic of rough vol (depending on the chosen H and $\eta$). But the performance is a concern: 10k paths * 7 years of daily steps will take a long time in Python. The correctness of the model is fine for an experimental setting, though it‚Äôs worth noting that no mean-reversion is present in this variance model (contrast with Heston‚Äôs mean reversion). $\xi_0$ stays as the expected variance, but realized volatility can wander lognormally. This is acceptable given the goal (stress-testing the RILA under a different vol dynamic), but one might question if the rough vol parameters $(H=0.1, \eta=1.5, \xi_0=0.04)$ are calibrated or just hypothetical. In the dissertation context, they are likely illustrative. RILA Payoff Logic: The scripts handling the RILA payoffs (7_rila_under_gbm.py, 9_rila_heston.py, 11_rila_under_roughvol.py) implement the buffered cap-and-floor payoff structure. We examine the logic in the single-term case first (Heston and rough vol): For a single-term RILA with initial investment $P_0$ (1000 in the code) over T years, cap = c (e.g. 50% = 0.5 as fraction) and buffer = b (e.g. 10% = 0.1):
Let $R = S_T/S_0 - 1$ be the total raw return of the index over the term.
If $R \ge 0$ (index rose), the payoff caps the gain at c: credited return = min$(R, c)$.
If $R < 0$ (index fell), but within the buffer (i.e. loss less than b), then no loss is applied: credited return = 0 (the buffer absorbs the loss).
If $R < 0$ and beyond the buffer (more than 10% down), then the loss beyond b is applied: credited return = $R + b$ (a negative number in this case, since $R$ is e.g. -20% and $b=10%$, credited = -10%). This effectively means the account loses only the portion exceeding the 10% buffered amount.
This logic is correctly coded in 9_rila_heston.py and 11_rila_under_roughvol.py. For example, in 11_rila_under_roughvol.py:
python
Copy
Edit
if ret >= 0:
    credited = min(ret, cap)
else:
    if abs(ret) <= buffer:
        credited = 0
    else:
        credited = ret + buffer  # loss beyond the buffer
final_account = initial_investment * (1 + credited)
This matches the intended payoff: capped upside, buffered downside. The code then collects the final account values for all simulated paths, obtains a discount rate for the term (via get_r_for_discounting) and discounts all payoffs back to present. The discounted values distribution is plotted as a histogram and summary stats (mean, std, VaR at 5% and 1%, min, max) are printed. The summary printout is a nice touch to quantify the risk (e.g. 5% VaR, worst case) of the RILA product under that model. One thing to verify is whether the risk-neutral simulation approach is appropriate here. If these simulations (Heston, rough vol) were done under $r-q$ drift (risk-neutral measure) and then discounted at $r$, the average of the discounted payouts should ideally equal the initial investment if the product is fairly priced (ignoring any fees or spreads). The code indeed uses $r-q$ as drift and discounts by the continuously compounded $r$ over T years. If the mean of the discounted final accounts is below 1000, that indicates the structured product offers less expected payoff than the cost (which could be due to the cap and buffer structure ‚Äì typically, a buffer protects the investor, while a cap limits upside, so depending on levels, the product could be cheaper than full equity exposure). The code prints the final mean, so one can check that to infer if the pricing is fair or if there‚Äôs ‚Äúvalue‚Äù to one side. All in all, the logic for payoff and discounting is implemented correctly. In 7_rila_under_gbm.py, the logic is very similar but tailored to the simpler GBM scenario with a 12% cap and 10% buffer. The code uses vectorized operations for efficiency: it reads all final path values from a CSV (simulated elsewhere), computes returns in one numpy array operation, and then uses np.where to apply the cap/buffer conditions across the entire returns array without an explicit Python loop. For example:
python
Copy
Edit
returns = (end_values - start_values) / start_values
credited = np.where(returns >= 0, np.minimum(returns, cap),
                    np.where(np.abs(returns) <= buffer, 0, returns + buffer))
final_accounts = initial_account * (1 + credited)
This kind of vectorized formulation (which the code uses implicitly by building final_accounts in a loop but could be done as above) is correct and more succinct. In practice, the GBM RILA script does loop over returns (for r in returns), applying the same logic, which is functionally correct but not as optimized as it could be. Given 10,000 paths, this loop is not a big issue (10k iterations in Python is fine). The end result ‚Äì distribution of present values ‚Äì is again plotted and summary stats printed. The code explicitly prints example returns and final accounts for a quick sanity check, which helps ensure the logic is working as intended (the printouts are prefixed with ‚Äú‚úÖ‚Äù indicating they verified a few sample calculations). Advanced (Annual Reset) RILA logic: In the 6_simulate_spx_gbm.py script, after simulating GBM paths, the code applies a year-by-year crediting logic, which is more complex:
The contract is broken into annual periods (here 6 years, term_years = 6 in the script).
Each year‚Äôs growth is measured (the script precomputes an array of annual returns for each path).
For each year, the account‚Äôs value is adjusted as follows:
A cap of 12% is applied to that year‚Äôs return.
A -10% buffer is applied to that year‚Äôs loss (meaning if the index fell more than 10%, the excess loss beyond -10% is applied; if it fell less than 10%, no loss that year).
The participation rate (which is 1.0, full participation) multiplies the credited return.
The account value is multiplied by (1 + credited_return) for that year.
Then an annual fee of 1% is subtracted (the account is multiplied by (1 - fee_rate)).
This process repeats annually (compounded).
The code does this in a loop over each year, but vectorizes across all paths within that loop. For example, within each year for year in range(term_years):
python
Copy
Edit
capped = np.minimum(returns[year], cap)
buffered = np.where(capped >= -buffer, capped, capped + buffer)
credited = np.where(capped >= -buffer, capped, buffered)
account_values *= (1 + participation * credited)
account_values *= (1 - fee_rate)
This snippet shows that for the given year, it takes the vector of returns for all paths that year, applies the cap and buffer conditions via np.minimum and np.where (effectively the same logic as before, but on each year‚Äôs returns), and then updates the entire vector of account values in one go. This is a efficient and correct implementation of the contract‚Äôs annual reset feature. The use of buffered and credited intermediate arrays improves readability. By the end of the loop, account_values holds the final account values of all paths after 6 annual crediting periods and fees. The script then calculates summary stats and plots the distribution similar to the other RILA scripts. This advanced logic is more realistic for certain real-world annuity products that lock in gains and apply fees yearly. The code‚Äôs correctness here is evidenced by how it separately handles negative and positive years and ensures the fee is taken each year. The structure avoids any path-by-path Python looping, which is great for performance. The output (distribution of account values) can be directly compared to the single-term distribution to see how annual locking and fees change the risk/return profile. Heston Model Calibration: The calibration scripts are crucial for ensuring the Heston model parameters can capture the observed IV surface. Both 6_heston_calibrate.py (local search) and 6_heston_calibrate_de.py (global DE search) define a calibration loss function that iterates over each option quote in the snapshot and accumulates squared differences between model-implied vol and market implied vol. Key aspects:
They load a snapshot of market data (presumably a CSV with columns strike, maturity, impl_volatility, etc.). The snapshot should contain options for one date across strikes and maturities (the earlier plotting script suggests 2018-06-01 is used).
Model pricing: For each option, the code calls carr_madan_call_price to compute the theoretical call price under a given set of Heston parameters. The Carr-Madan Fourier method is a sound choice for Heston pricing, and the implementation in heston_pricing_carr_madan.py appears standard (with integration limits and an damping factor Œ± to ensure convergence). The calibrator passes in an alpha of 2.5 and a reasonably fine integration grid (N=1000, u_max=100 in the local, and slightly smaller N=800, u_max=80 in the DE version for speed). These settings trade off precision vs. performance; they seem chosen to avoid integrand blow-ups and reduce computational load, which is prudent.
After obtaining a model call price, it converts that price to an implied vol by calling bs_implied_vol. This function likely inverts the Black-Scholes formula (probably via a numerical method). The calibration uses implied vol error rather than price error. Working in IV space can be advantageous because it normalizes the errors across strikes and maturities (since option prices can vary by orders of magnitude, whereas an implied vol of 10% vs 12% is a more uniform comparison). The code takes care to handle edge cases: if the solver returns NaN or an implausible vol (<0.0001 or >5, which indicates a failure to converge or nonsensical output), it assigns a fixed large error (1e2 or 100) for that quote. This is a good defensive programming practice to avoid one bad option quote crashing the entire calibration or skewing the optimizer.
A penalty is added in the loss function for ‚Äúunphysical‚Äù parameter sets before any pricing is done. The code checks bounds: $v_0$ not negative or too large (cap at 2.0, which is 200% variance ‚Äì extremely high, basically preventing absurd starting volatility), $kappa$ > 0 (must be positive for mean reversion; capped at 10 or 12), $theta$ (long-run var) positive but not too high (<=2.0), $\sigma_v$ > 0 (vol-of-vol, capped at 5.0 to prevent extreme oscillations), and $\rho$ in (-0.999, 0.999). If any of these conditions fail, the function returns a very large loss (1e6), effectively invalidating that parameter set. This keeps the optimizer within a reasonable region of the parameter space and improves stability. It‚Äôs a form of soft constraints beyond the explicit bounds given to the optimizer.
The optimizer: In the local case, SciPy‚Äôs minimize with L-BFGS-B is used, with explicit parameter bounds matching or slightly tighter than the penalty conditions. The initial guess is [0.04, 2.0, 0.04, 0.3, -0.7] (these are typical values: 4% initial var, high mean reversion speed 2, 4% long-run var, 30% vol-of-vol, -0.7 correlation). The script sets disp: True so intermediate steps may print, and a max of 200 iterations (which should be enough for convergence if it finds a good valley). In the DE case, SciPy‚Äôs differential_evolution is used with the same bounds (slightly wider kappa upper bound 5.0 vs 10 in local, but effectively the same order) and a max of 80 generations. By default, DE will evaluate many candidate points per generation (pop size ~15*dim by default, dim=5, so ~75 candidates per gen), so 80 generations is 6000 evaluations ‚Äì quite intensive given each evaluation loops through all options and does an integration. The code sets disp=True to see progress and polish=True to let DE perform one final local optimization on the result. This polishing step likely uses L-BFGS-B internally to fine-tune the best candidate found by DE.
After optimization, both scripts print the calibrated parameters clearly and save them to a CSV file for record.
The behavior of these calibration scripts should be as follows: The DE version will search globally, hopefully finding a near-global minimum of the error surface (which can be multi-modal for calibration problems). The local version, if started from a decent initial guess, may converge faster and can be used to fine-tune or test the results. The presence of two scripts suggests the author compared the two approaches (e.g. to ensure the local optimum wasn‚Äôt a poor fit by comparing to the global result). Correctness of calibration: The approach of calibrating to implied vol is standard for option surface fitting. One subtle point: calibrating to an entire surface (multiple maturities & strikes) typically requires also specifying the model‚Äôs interest rate r and dividend q to price options. Here, they fixed r=0.02, q=0.01 inside the script ‚Äì these may correspond to roughly the risk-free and dividend yield for the snapshot date. Since they are using implied yields from data in simulations, one might expect they use the same yield in calibration, but instead they hard-coded these small values. It could be an approximation or a simplification (2% risk-free, 1% dividend). Ideally, one would use the term structure from the data for each option‚Äôs maturity (the data snapshots likely contain or assume certain rates). This is a minor potential inconsistency; however, because they calibrate to implied vols (which are already net of fwd price effects), using a flat r and q likely has minimal impact on the implied vol fit. It‚Äôs mostly a concern if one were matching prices exactly. The integration via Simpson‚Äôs rule should be adequate for accuracy, given Œ±=2.5 damps the integrand and u_max=100 (or 80) covers enough integration range for typical maturities (exceeding a few years). If anything, those parameters might need tweaking for very long maturities or extreme strikes, but since the snapshot is a single date (maybe with options up to 1-2 years out), it‚Äôs fine. Calibration results and completeness: The code does not explicitly plot the fit or compute error metrics beyond the average loss, but a user can inspect the printed loss value and the output CSV with parameters. It would be straightforward to extend the script to, say, print the final loss or create a quick plot of model vs market IV after calibration for verification. As is, it assumes the user will trust the final numbers or plug them back into a pricing routine to visualize the fit. Given this is for a dissertation, likely the text includes a figure or table of the calibrated parameters and perhaps a chart of model vs market smiles, even if the code doesn‚Äôt directly produce it. In testing the code‚Äôs behavior: the presence of print(f"‚úÖ Loss: ... for params: ...") inside the loss function means that during optimization, every function evaluation prints a line. This can be very verbose (hundreds or thousands of lines of output) ‚Äì likely the author used it to monitor progress. It can slow down the optimization, but since this is an offline analysis, it‚Äôs tolerable. One might normally disable such detailed printing once things are working, but it‚Äôs not harmful except for clutter. To summarize, each module‚Äôs behavior aligns with expectations: the data scripts produce cleaned inputs; the visualization confirms data patterns; the simulation scripts generate scenarios under different models (with attention to drift and numeric stability); the RILA scripts correctly transform those scenarios into payoff distributions; and the calibration scripts attempt to tune model parameters to market data using reasonable techniques. There were no obvious mathematical errors in the code ‚Äì the formulas used are standard, and where approximations are made (e.g. abs for variance, discrete time steps, etc.), they are documented or justified by comments.
3. Suggestions for Code Improvements (Efficiency, Modularity, Maintainability)
While the code is overall correct and yields the intended results, there are several areas where improvements could enhance performance, clarity, and reusability:
Performance Optimizations:
Rough Volatility Simulation: The nested loop approach is the most urgent inefficiency. Generating 10,000 independent fBM paths will inherently take time, but the inner loop over 1764 time steps for each path can be eliminated. A vectorized approach could be: generate the fBM path W_H (length n_steps) and an independent normal path dW for each path, then compute the entire price path via cumulative products. For example, once v_t is computed for a path, the price increments $\Delta \ln S_j = (\mu - 0.5 v_j),dt + \sqrt{v_j},\Delta W_j$ can be formed as arrays and exponentiated cumulatively to get $S_t$. Using NumPy‚Äôs vectorized operations (and perhaps np.cumsum or np.cumprod) would shift the heavy work into C and could drastically speed up the simulation. Alternatively, one could leverage parallelization (e.g. joblib or multiprocessing) to distribute path simulations across cores, given each path is independent. Even reducing n_paths for quick testing and providing a parameter to increase it for final runs would help manage performance.
Heston Simulation: This is already vectorized in time. One micro-optimization: generating Z1, Z2 as standard normals for the whole grid is fine, but memory heavy at shape (1764, 10000). If memory became an issue, generating in chunks or streaming could be considered. However, given typical modern memory, this is acceptable. The simulation could also benefit from more advanced schemes (e.g. QE scheme for Heston) for accuracy, but Euler is adequate for the purpose here.
Calibration: The calibration is computationally expensive by nature. If it becomes a bottleneck, one idea is to vectorize the pricing of all options with a single FFT (as in Carr-Madan you can in principle price a whole strike grid at once by FFT). The current approach calls pricing and implied vol computation for each strike in a Python loop, which is fine for maybe tens of options, but if the snapshot has hundreds of options, that‚Äôs hundreds of integrations per evaluation. Some improvements:
Reduce the frequency of print in the loss function. Currently it prints for every parameter set tried, which slows down each function call. In the SciPy minimize, this can be turned off or toned down (e.g. print only every Nth evaluation or only when loss improves). This will speed up the calibration especially in differential_evolution where many evaluations occur.
Use NumPy arrays to gather option parameters and perhaps compute the characteristic function once per evaluation for all strikes on a grid, though adapting Carr-Madan for multiple strikes in one go might be complex because it naturally produces one price per transform. Another simpler trick: because the code calibrates to implied volatilities, one could weight errors by option vega or use a different metric (but that‚Äôs more about accuracy than speed).
If calibration is too slow, a compromise is to calibrate on a subset of the surface (e.g. 1-2 maturities, a reduced strike set) initially, or use the DE result as a starting point for fewer local refinements. The provided approach is already quite thorough (global + local).
Modularity and Reusability:
There is some repeated logic across scripts that could be consolidated into functions or a small library of helpers:
RILA payoff logic: The buffer-and-cap calculation appears in at least three scripts in slightly different forms. This could be abstracted into a function, e.g. apply_rila_cap_buffer(returns, buffer, cap) that returns an array of credited returns given an array of raw returns. This function could internally handle both the single-term case (vector input of final returns) and, if needed, the per-year case. For example, it could be written to accept a NumPy array and use vectorized operations as demonstrated in the GBM case. Centralizing this logic would ensure consistency (right now, all three implementations are consistent, but maintenance is easier if you only have one place to update if the formula changes or if you want to add a feature like toggling reset_annually).
Parameter configuration: Many scripts define variables like S0, n_paths, T, cap, buffer at the top. It might be useful to have a single configuration or at least collect them in one place for clarity. For example, the cap and buffer differ between GBM and Heston scripts (12% cap vs 50% cap), which is fine as they were probably exploring different scenarios. But if one wanted to systematically compare models under the same cap/buffer, you‚Äôd have to manually ensure each script uses the same values. A better approach could be to factor these into a config section or use command-line arguments to pass them. Since this is a research repo, an alternative is to document clearly at the top of each script what scenario it represents.
Interest rate handling: The get_r_minus_q and get_r_for_discounting are already modular in utils.py. One improvement is to integrate the risk-free data cleaning step into get_r_for_discounting. For instance, currently get_r_for_discounting(date, rate_file) likely reads the yield curve CSV and interpolates or picks the appropriate rate. In the RILA scripts, they pass 'Interest_Rate_Curves_2018_2023.csv' (the uncleaned file). But earlier, they created a cleaned file '..._CLEANED.csv'. Possibly get_r_for_discounting itself handles the reading and perhaps picks the 7-year rate. It would be clearer if get_r_for_discounting accepted the term (T=7) as an argument, or explicitly took the cleaned file. Modularizing these functions further (e.g. a function to read and cache the curve data, to avoid re-reading CSV each time for multiple paths) could improve efficiency if these functions are called repeatedly in loops (though in the code, they‚Äôre called only once per script, so it‚Äôs fine).
Snapshot generation: There is no script to generate SPX_Snapshot_2018-06-01.csv from the cleaned data. Adding a small utility to filter the SPX_Options_CLEANED.csv for a given date and save it would improve reproducibility. It can be as simple as a few lines with df[df['date']==snapshot_date] and a to_csv. This ensures that someone running the repo from scratch can regenerate the snapshot needed for calibration and surface plotting. It also avoids any ambiguity about which options were included (e.g., did they filter for a certain maturity range or take all available on that date?).
Packaged as library or not: Given this is a dissertation project, having separate scripts is acceptable. But if one were to transform this into a maintainable codebase, one might create a Python package structure with modules (data_prep.py, models.py, calibration.py, etc.) and use function calls instead of hardcoding file I/O in every script. For example, a function simulate_heston(S0, v0, ..., n_paths, T) returning arrays would allow easier unit testing and re-use (e.g., one could simulate and then directly call a RILA payoff function on the result without writing to CSV in between). The current approach writes intermediate results to CSV (like simulated paths) and then reads them in the RILA scripts, which works but is file-system heavy. Passing data in memory (or using pickle/npz if persistence is needed) could be more efficient. This suggestion is more about design: decoupling the simulation from the analysis. In practice, the file output approach is fine for a linear workflow.
Maintainability and Clarity:
Comments and Documentation: The code is peppered with brief comments and emoji markers that make it quite readable and even enjoyable to follow. Adding a top-of-file docstring or header comment for each script explaining its purpose (in human language) would benefit future readers. For instance, at the top of 8_simulate_spx_heston.py, a short note like ‚Äú# Simulate SPX under Heston model and save paths for RILA analysis‚Äù would immediately tell someone scanning the files what it does. Some scripts do have a brief descriptive comment at the top (e.g. # Cleans the merged dataset... in 2_clean_and_prepare.py), which is good. It would be great to ensure each script has such an intro.
Parameter naming and units: In data cleaning, they used maturity_days and maturity_years which is clear. In simulation, variables like dt = T / N assume N is number of steps (which it is). Perhaps ensure consistency: they use N = 252 * T and dt = T/N (so if T=7 years, N=1764 trading days, dt ‚âà 0.00397 in year units). This is correct. Just ensure readers know dt is in years (since T is in years and N is #steps). A tiny comment on that could help avoid confusion.
The code currently mixes analysis and output generation (plots, CSVs) within the same script. This is fine for this context, but if maintainability was a concern, one could separate the ‚Äúcompute‚Äù part from the ‚Äúproduce output‚Äù part. For example, have a library function to compute the distribution of RILA outcomes given paths, and a separate block to do plotting. This would allow easily changing the way output is presented without touching the core logic.
Magic numbers: There are a few places with ‚Äúmagic numbers‚Äù that are actually justified but could be more explicitly referenced. For example, in calibration, returning 1e6 as loss for invalid params, or error = 1e2 for an option pricing failure. These are effectively penalty constants. It might be clearer to define them as named constants (e.g. PENALTY_LARGE = 1e6, PENALTY_IV = 1e2) to convey intent. Also, the bounds chosen (0.01‚Äì0.2 for v0, etc.) seem to be based on some prior knowledge. A comment about why those ranges (e.g. ‚Äú# restrict v0 and theta to [0.01, 0.2] as typical vol^2 range 1%-20%‚Äù) would educate the reader.
Output files organization: The outputs are saved in categorized subfolders (Output/plots, Output/simulations, Output/surfaces). This is a good practice for organization. One improvement is to ensure the file names are descriptive and maybe include parameter values if relevant. For instance, the calibrated params CSV is saved with the snapshot date in the filename ‚Äì good. The simulation paths CSVs have clear names (SPX_Heston_paths.csv, etc.). The distribution plots for RILA are named in a way that identifies the model (e.g., va_distribution_heston.png, ...roughvol.png, ...gbm_rila_2018-01-03.png for GBM). These naming conventions are actually already quite helpful. To further maintain consistency, one could include the term years or other scenario info if multiple scenarios were run. But given each script uses fixed terms (mostly 7 years except the GBM advanced case which was 6), it‚Äôs okay.
Robustness and Edge Cases:
The calibration code could possibly face edge cases if the snapshot data includes deep in-the-money or far-out-of-the-money options with almost zero or very high implied vol. The code tries to handle weird implied vol outputs by penalizing them. Another robust approach could be to cap the model implied vol error per point, or use a slight epsilon in np.abs(ret) <= buffer comparisons to avoid floating precision issues. But those are minor (10% expressed as 0.1 is exact in binary, so fine).
The simulation seeds are all fixed to 42 at the start of each simulation script. This ensures reproducibility. If the user wanted to run multiple scenarios or Monte Carlo batches, they might parameterize the seed. As it stands, the fixed seed is good for reproducibility (the same ‚Äúrandom‚Äù paths each run) ‚Äì which is likely intentional so that one can compare outcomes across model assumptions on an apples-to-apples basis (though here each model has its own randomness, not directly comparable path-wise, but at least results are consistent run to run).
One potential issue: The drift mu = get_r_minus_q(...) is fetched using the original (not cleaned) interest rate file in the rough vol script. If the cleaned file (with only needed columns) is slightly different (it might exclude some structure), there is a small inconsistency: the Heston simulation used 'Interest_Rate_Curves_2018_2023_CLEANED.csv', whereas the rough vol uses 'Interest_Rate_Curves_2018_2023.csv' (the uncleaned). It‚Äôs likely get_r_minus_q doesn‚Äôt need the cleaned version because it might itself filter or ignore extra columns. But for consistency, one could use the cleaned file everywhere after preparing it. This is a minor point; functionally it probably makes no difference because the uncleaned file just has extra columns that are not used.
Similarly, ensure that the interest rate used for discounting in RILA (get_r_for_discounting) corresponds to the term of the product. Ideally, that function is picking the 7-year zero-coupon rate from the curve on 2018-01-03. The code passes the date '2018-01-03' and the curve file, presumably to extract the rate for a 7-year maturity (maybe it finds the closest maturity_days = 252*7 ‚âà 1764 in the curve data). It‚Äôs a bit implicit. To avoid any mismatch, one might explicitly pass T into get_r_for_discounting or ensure the function is documented to fetch e.g. the longest available rate or do an interpolation. Since we haven‚Äôt seen the implementation, we trust that it returns a scalar continuous rate that they then use in exp(-r_discount * T).
In conclusion on improvements: Focusing on vectorization (especially for rough vol), eliminating redundant code by abstracting common functionality (RILA payoff), and improving structure for reusability (perhaps using functions and a main harness) would make the code more efficient and easier to work with for future modifications. None of these changes require altering the fundamental logic, so they can be implemented without affecting the validity of results. They would primarily reduce runtime and code volume. Given that this is a research codebase, the priority might not have been on maximum efficiency or minimal repetition, but rather on clarity and getting results. The suggestions above aim to retain clarity while moving it towards a more production-quality style.
4. Suggestions for Additional Experiments or Validations
The project as is already covers a broad range (data analysis, model simulation, product payoff evaluation, calibration). There are a few follow-up experiments or validation steps that could strengthen the analysis:
Validate Calibration Fit: After calibrating the Heston model, it would be valuable to verify how well it reproduces the market IV surface. A simple extension would be to plot the market vs model implied volatility for each option in the snapshot (perhaps by overlaying model curves on the market IV scatter plot). This could be done by taking the calibrated parameters, repricing the options, and comparing implied vols. Even printing the root-mean-square error or maximum error would quantify the goodness of fit beyond the mean squared error. For example, identify any region (short maturity, far OTM, etc.) where the model struggles, to give insight into model limitations. Since the code to price and get IV is already written, it wouldn‚Äôt be hard to do this as a post-calibration step.
Calibration on Multiple Dates: The code focuses on one snapshot (2018-06-01). It might be interesting to calibrate the Heston model on a few different dates (e.g. one per year from 2018 to 2023, or before/after major volatility events like Feb 2020) to see how the parameters change over time. This would reveal the model‚Äôs stability and how market conditions affect parameters (e.g. does $v_0$ jump during high-vol periods, does $\rho$ remain strongly negative consistently, etc.). This could be done by looping the calibration over a list of dates. It would require having snapshots for those dates or filtering the main dataset by date on the fly. While time-consuming, using the DE approach on multiple dates might be too slow; a quicker approach is to use the previous day‚Äôs calibration as the initial guess for the next day‚Äôs local calibration. This kind of experiment would give a term structure of Heston parameters or a sense of parameter dynamics.
Compare RILA outcomes across models: The code already produces distributions of final account values under three different stochastic models (GBM, Heston, rough vol). A natural extension is to compare these distributions side by side. For example, one could compute the mean and VaR under each model to see which model predicts a worse downside or higher upside for the RILA. Or create a plot that shows the cumulative distribution functions of the outcomes for each model on one graph. This would highlight the impact of stochastic volatility (both conventional and rough) on the risk profile of the product, versus the simpler GBM assumption. It could also be insightful to see how the pricing might differ: since all are risk-neutral (in theory), the mean present value should be similar if calibrated to the same market. However, the distribution shapes (hence VaR, etc.) will differ, which is important for risk management. The groundwork is all there (the scripts print means, VaRs). It‚Äôs a matter of aggregating or plotting them together for comparison. For instance, a table could be made of [Mean, Std, 5% VaR, 1% VaR] for each model‚Äôs RILA simulation.
Stress Testing Buffer/Cap Levels: Another experiment could be to vary the RILA parameters themselves. The code currently uses one buffer and cap level per scenario (10% buffer with either 50% or 12% cap). It might be interesting to see how outcomes change if the cap or buffer is different. For example, what if the buffer were larger (e.g. 20%) or the cap smaller? This could show the trade-off between protection and upside. This can be done by parameterizing the RILA logic. If the code were refactored to have the buffer and cap as easily adjustable parameters at the top of the script or via a function, one could run multiple simulations (or simply loop over a set of cap values in one run) and record the results. A simple sensitivity analysis could be: fix a model (say Heston) and simulate the final outcomes for cap = 30%, 50%, 70% (keeping buffer 10%) and for buffer = 0%, 10%, 20% (keeping cap 50%). Compare means and VaRs. This would effectively simulate different product designs.
Inclusion of Empirical Data for Validation: If historical SPX paths or realized outcomes are available, one could attempt to validate the models or the RILA payoff using historical data. For example, using actual S&P 500 returns from 2018‚Äì2025 (7-year period) to see what the actual RILA payoff distribution might have been, and compare that to model predictions. This might be beyond scope (and historical 7-year overlapping periods are limited in sample), but it grounds the analysis. Another form of empirical validation: check the initial pricing of the RILA structure. Since the product is essentially an option package (a combination of a call spread and a floor, etc.), one could attempt to price it directly from the option market data and compare to the model simulation price (mean PV). If the mean PV from risk-neutral simulation equals the cost of a 7-year structured note (which could be approximated by combining a zero-coupon bond and options), that gives confidence in the approach. This is a complex task since 7-year options on SPX might not be traded (except LEAPS up to 2-3 years, or via extrapolation).
Rough Volatility Calibration: A very advanced extension would be to calibrate a rough volatility model to market data (e.g. using the Rough Heston model parameters H, etc.). This is non-trivial as it involves fractional calculus and typically requires a specialized calibration approach. It might be outside the scope of the dissertation, but given that a rough vol simulation was done, a natural question is: can a rough vol model fit the option surface better than Heston? Doing this would be a research contribution in itself. However, even a qualitative experiment like: ‚Äúwe choose H=0.1, Œ∑=1.5 to roughly match short-term vol dynamics‚Äù and see if the simulated implied vol surface from those paths resembles the market‚Äôs could be insightful. Currently, the rough vol simulation is used only for RILA payoff distribution, not for option pricing. One could attempt to extract an implied vol smile from rough vol simulations (though computationally heavy, it can be done by pricing options on the simulated paths) to see if it produces the steep skew typical of SPX. This would validate whether rough volatility provides a better explanation for implied vol skews.
Risk-Neutral vs Real-World: The analysis implicitly uses risk-neutral simulations for pricing the RILA. Another experiment is to consider real-world (P-measure) scenarios. For instance, use an estimated equity risk premium and higher drift in the GBM or Heston model to simulate actual expected outcomes for an investor, as opposed to risk-neutral pricing. Then discounting by risk-free still (if evaluating from pricing perspective) versus not discounting (if looking at realized growth). This can highlight the difference between pricing a product and the actual distribution an investor might experience. It adds depth by separating the ‚Äúfair value‚Äù analysis from the ‚Äúwhat if I hold this investment‚Äù analysis. Implementation-wise, that means using mu = r - q + \text{risk premium} in simulations, which the current utility could be adapted to (perhaps by inputting a different rate or adjusting the get_r_minus_q function).
Alternative Calibration Metrics: One validation step for calibration is to check the stability of the solution. For example, if you run the local calibration with a different initial guess, do you get the same result? Or if you change the weight (like use sum of absolute errors instead of squared, or give more weight to short maturities), does it drastically change parameters? These kinds of tests ensure the calibration result is robust and not an artifact of the specific error norm. The code could be extended to try multiple initial guesses or to perturb the fitted solution and see if the optimizer returns to it (which would indicate a strong minimum).
Many of these suggestions go beyond what‚Äôs strictly necessary, but they could provide additional evidence and richness to the study:
For instance, comparing distributions answers the ‚Äúso what‚Äù of having different models ‚Äì it shows the effect on risk metrics.
Calibrating over time or stress testing design parameters could be very useful in a discussion about the product‚Äôs performance under different conditions.
Empirical checks ground the models in reality, ensuring that all the fancy modeling connects back to actual market or investor experience.
Given the time and scope of a dissertation, the most practical additions might be comparing model outcomes (distribution comparisons) and visualizing calibration fit. These can be done with moderate effort and yield clear illustrations. More involved tasks like rough vol calibration might be future work, but mentioning them or attempting a limited version could be impressive if feasible.
5. Summary and Prioritized Recommendations
In summary, the repository‚Äôs code is comprehensive and mostly well-structured for the tasks at hand. It successfully covers data processing, visualization, simulation under multiple models, and product payoff analysis, capped by model parameter calibration. The results produced (clean data, volatility plots, simulation outputs, distribution histograms, calibrated parameters) address the key questions about how a RILA product behaves under different volatility assumptions and how well a model like Heston can fit the market. Strengths:
The code is accurate in its computations: formulas for option pricing and path simulation are correctly implemented. The RILA payoff logic aligns with the contract definition. Calibration uses established numerical methods with sensible safeguards.
It is fairly readable: the use of clear variable names (e.g. maturity_days, final_accounts), inline comments, and structured output messages (with emojis for status) makes it easy to follow the computational narrative.
The workflow is modular in concept (each script does one main thing), which is good for clarity. Intermediate results are saved to files, allowing inspection at each stage.
Results appear plausible and reproducible due to fixed random seeds and logged outputs. This is crucial in a research context so that figures in the dissertation can be regenerated if needed.
Areas for Improvement (with priorities):
Performance (High Priority) ‚Äì The most immediate improvement would be to optimize the rough volatility simulation. As discussed, vectorizing the inner loop or parallelizing the simulation would drastically cut down runtime. Given that this simulation could be the slowest part of the whole pipeline, addressing it is important if one needs to run it multiple times or with higher path counts. This is a high-priority fix for efficiency. The Heston simulation and RILA logic are already reasonably efficient, but rough vol stands out. If optimization time is limited, one could also consider reducing n_paths or using a coarser time step for initial experiments, then ramping up for final results ‚Äì though those are more like workarounds than code improvements.
Code Refactoring for Reuse (Medium Priority) ‚Äì Consolidating repeated logic (especially the RILA payoff application) into a utility function would reduce code duplication and potential inconsistencies. While the current duplicated code is consistent, having one source of truth for that logic is safer for future modifications (say, if the product were modified to include a floor or different structure). This refactor also makes the analysis more extensible ‚Äì for instance, one could easily call the payoff function on any array of returns, enabling the sensitivity analysis mentioned above. Similarly, a small refactor to allow passing parameters (cap, buffer, fee) into the simulation scripts (instead of them being hardcoded) would allow systematic experiments without manual editing of each script.
Better Integration of Data Prep and Usage (Medium Priority) ‚Äì Ensuring that all parts of the pipeline use the same cleaned data consistently is important for reproducibility. This includes possibly automating the snapshot extraction and using the cleaned interest rate curve file uniformly. While this doesn‚Äôt change results, it prevents confusion. This is of medium priority because it affects ease-of-use more than the output‚Äôs correctness.
Documentation and Clarity (Medium Priority) ‚Äì Adding more descriptive comments or a short README that outlines how to run the analysis from start to finish would be beneficial. Currently, a user must infer the order (likely: run merge, clean, riskfree prep, then perhaps generate a snapshot, then calibration or simulations, then RILA analyses). A README could list the sequence and any prerequisites (like where to get the raw data files). Within code, clarifying units and assumptions (e.g. ‚Äúusing risk-neutral measure for simulations‚Äù) would strengthen understanding. While the code is for the author‚Äôs dissertation (so they know the context), adding documentation is good practice and helps any future readers or evaluators. This is medium priority ‚Äì not affecting results, but enhancing comprehension.
Extend Analysis with Comparative Plots or Summary (Low to Medium Priority) ‚Äì Not strictly code improvement, but adding a script or notebook to aggregate results ‚Äì for example, to compare the RILA outcome distributions from different models on one chart, or to plot the calibrated vs market IV ‚Äì would elevate the analysis. This could even be done as part of the dissertation document rather than code, but having code produce such summary visuals ensures accuracy. If time permits, this is a worthwhile addition as it ties everything together.
Maintainability for Future (Low Priority for now) ‚Äì If this codebase were to be used beyond the dissertation (e.g., for further research or a publication), consider reorganizing it into a more conventional project structure (with a main driver script or notebook, functions in modules, etc.). Right now, maintainability is fair for a one-off analysis (each script can be run independently), but scaling it or iterating on it could lead to repetitive changes. This is a lower priority in the context of completing the project, but worth keeping in mind.
Testing and Verification (Low Priority) ‚Äì Adding a few assertions or simple unit tests can help ensure nothing breaks as improvements are made. For instance, after data cleaning, assert that no impl_volatility NaNs remain; after simulation, assert that shapes of output arrays are correct, etc. Given this is research code, formal tests are not expected, but internal consistency checks (some of which are done via prints) are helpful.
Prioritization Rationale: Performance is ranked highest because a slow simulation can impede progress or analysis, especially if adjustments require reruns. Next, refactoring repeated logic and data consistency are about reliability and ease of performing additional analyses ‚Äì moderately important if the project evolves. Documentation improvements are always beneficial, especially if the dissertation advisor or peers need to run the code. The comparative analysis suggestion is more about extracting maximal insight (which is often the difference between a good and a great project); it‚Äôs prioritized medium because while not strictly required, it can substantially improve the communication of results. In conclusion, the codebase is in good shape for a research project. By implementing the above improvements, one can make it run faster, be easier to adapt, and provide deeper insights, all of which would enhance the robustness and reproducibility of the findings. The recommendations, summarized:
Optimize rough vol simulation (e.g. vectorize inner loop) ‚Äì to handle large path simulations efficiently.
Refactor common logic (RILA payoff) into utility functions ‚Äì to avoid duplication and ease changes.
Use consistent cleaned data and automate snapshot creation ‚Äì to ensure reproducibility and reduce manual steps.
Augment documentation and comments ‚Äì to clarify usage and underlying assumptions for each module.
Add comparative analyses and sanity-check plots ‚Äì to validate model fits and highlight differences between model predictions.
Restructure for modularity (if future use expected) ‚Äì to treat this code as a reusable toolkit rather than single-use scripts.
By addressing these, the project will not only demonstrate correct results but also exemplify strong coding practices and analytical rigor, which is especially important in an academic dissertation context.